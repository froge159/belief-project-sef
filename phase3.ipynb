{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMMMaYG0JjRcay5US40zYwk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/froge159/belief-project-sef/blob/main/phase3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thzs-ddCMwAJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import random\n",
        "import csv\n",
        "# install bitsandbytes and restart"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zY6Xcqz2VyoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(path):\n",
        "  pass\n",
        "dreaddit = get_dataset('/content/drive/MyDrive/SEF/data/dreaddit.csv')\n",
        "# dreaddit_prompts ="
      ],
      "metadata": {
        "id": "XkCAcbVqVzqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, padding=True, truncation=True, model_max_length=512)\n",
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.truncation_side = \"right\"\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    dtype=torch.float16,\n",
        ")\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "YZKm7wH4V90W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = model.config.num_hidden_layers\n",
        "num_heads = model.config.num_attention_heads\n",
        "head_dim = model.config.hidden_size\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "_DoztyuvWDhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Mean-Pooled Activations"
      ],
      "metadata": {
        "id": "WbrmK-pwWPXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_mean_pooled_hidden(prompts, batch_size=8):\n",
        "  all_pooled = []\n",
        "  for i in range(0, len(prompts), batch_size):\n",
        "    batch = prompts[i:i+batch_size]\n",
        "    inputs = tokenizer(\n",
        "        batch,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "    hidden_states = outputs.hidden_states[1:]\n",
        "    attention_mask = inputs[\"attention_mask\"] # [bs, seq_len]\n",
        "    mask = attention_mask.unsqueeze(-1)\n",
        "    lengths = attention_mask.sum(dim=1).unsqueeze(-1)\n",
        "\n",
        "    pooled_layers = []\n",
        "    for hs in hidden_states:\n",
        "        # hs: [bs, seq_len, hidden_dim]\n",
        "        masked_sum = (hs * mask).sum(dim=1)     # [bs, hidden_dim]\n",
        "        mean_pooled = masked_sum / lengths      # [bs, hidden_dim]\n",
        "        pooled_layers.append(mean_pooled)\n",
        "\n",
        "    # Stack layers: [bs, num_layers, hidden_dim]\n",
        "    batch_pooled = torch.stack(pooled_layers, dim=1)\n",
        "    all_pooled.append(batch_pooled.cpu())\n",
        "\n",
        "  return torch.cat(all_pooled, dim=0)"
      ],
      "metadata": {
        "id": "slNadv0sWScD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_pooled = extract_mean_pooled_hidden(dreaddit_prompts)"
      ],
      "metadata": {
        "id": "mOiTK5rzWUQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Linear Probe"
      ],
      "metadata": {
        "id": "cM6lUQM3Wrt-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FtxibXhGWt1j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}