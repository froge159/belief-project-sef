{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/froge159/belief-project-sef/blob/main/phase3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thzs-ddCMwAJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "import pandas as pd\n",
        "# install bitsandbytes and restart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zY6Xcqz2VyoZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkCAcbVqVzqa"
      },
      "outputs": [],
      "source": [
        "dreaddit = pd.read_csv('/content/drive/MyDrive/SEF/data/dreaddit.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZKm7wH4V90W"
      },
      "outputs": [],
      "source": [
        "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, padding=True, truncation=True, model_max_length=512)\n",
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.truncation_side = \"right\"\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    dtype=torch.float16,\n",
        ")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DoztyuvWDhm"
      },
      "outputs": [],
      "source": [
        "num_layers = model.config.num_hidden_layers\n",
        "num_heads = model.config.num_attention_heads\n",
        "head_dim = model.config.hidden_size\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbrmK-pwWPXN"
      },
      "source": [
        "# Extract Mean-Pooled Activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slNadv0sWScD"
      },
      "outputs": [],
      "source": [
        "def extract_mean_pooled_hidden(prompts, batch_size=8):\n",
        "  all_pooled = []\n",
        "  for i in range(0, len(prompts), batch_size):\n",
        "    batch = prompts[i:i+batch_size]\n",
        "    inputs = tokenizer(\n",
        "        batch,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "    hidden_states = outputs.hidden_states[1:]\n",
        "    attention_mask = inputs[\"attention_mask\"] # [bs, seq_len]\n",
        "    mask = attention_mask.unsqueeze(-1)\n",
        "    lengths = attention_mask.sum(dim=1).unsqueeze(-1)\n",
        "\n",
        "    pooled_layers = []\n",
        "    for hs in hidden_states:\n",
        "        # hs: [bs, seq_len, hidden_dim]\n",
        "        masked_sum = (hs * mask).sum(dim=1)     # [bs, hidden_dim]\n",
        "        mean_pooled = masked_sum / lengths      # [bs, hidden_dim]\n",
        "        pooled_layers.append(mean_pooled)\n",
        "\n",
        "    # Stack layers: [bs, num_layers, hidden_dim]\n",
        "    batch_pooled = torch.stack(pooled_layers, dim=1)\n",
        "    all_pooled.append(batch_pooled.cpu())\n",
        "\n",
        "  return torch.cat(all_pooled, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOiTK5rzWUQ_"
      },
      "outputs": [],
      "source": [
        "mean_pooled = extract_mean_pooled_hidden(dreaddit['text'].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM6lUQM3Wrt-"
      },
      "source": [
        "# Train Linear Probe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtxibXhGWt1j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "\n",
        "def train_linear_probe(mean_pooled, labels, n_splits=5, seed=42):\n",
        "    \"\"\"\n",
        "    Trains a linear probe for each layer using cross-validation.\n",
        "    Args:\n",
        "        mean_pooled: torch.Tensor [n, num_layers, hidden_dim]\n",
        "        labels: array-like, shape [n]\n",
        "        n_splits: int, number of CV folds\n",
        "        seed: int, random seed\n",
        "    Returns:\n",
        "        pd.DataFrame with metrics for each layer\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    X_all = mean_pooled.detach().cpu().numpy()\n",
        "    y_all = np.array(labels, dtype=float)\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "    results = []\n",
        "    for layer_idx in range(X_all.shape[1]):\n",
        "        X_layer = X_all[:, layer_idx, :]\n",
        "        y_true_all = []\n",
        "        y_pred_all = []\n",
        "        for train_idx, test_idx in kf.split(X_layer):\n",
        "            X_train, X_test = X_layer[train_idx], X_layer[test_idx]\n",
        "            y_train, y_test = y_all[train_idx], y_all[test_idx]\n",
        "            model = LinearRegression()\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "            y_true_all.append(y_test)\n",
        "            y_pred_all.append(y_pred)\n",
        "        y_true_all = np.concatenate(y_true_all)\n",
        "        y_pred_all = np.concatenate(y_pred_all)\n",
        "        if np.std(y_pred_all) == 0 or np.std(y_true_all) == 0:\n",
        "            pearson = np.nan\n",
        "        else:\n",
        "            pearson = np.corrcoef(y_true_all, y_pred_all)[0, 1]\n",
        "        r2 = r2_score(y_true_all, y_pred_all)\n",
        "        mae = mean_absolute_error(y_true_all, y_pred_all)\n",
        "        results.append({\n",
        "            \"layer\": layer_idx,\n",
        "            \"pearson\": pearson,\n",
        "            \"r2\": r2,\n",
        "            \"mae\": mae,\n",
        "        })\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Call the function and display results\n",
        "test_results = train_linear_probe(mean_pooled, dreaddit[\"belief_score\"].values)\n",
        "test_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Validation Analyses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Probe Validity Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shuffle labels and run probe\n",
        "shuffled_labels = dreaddit[\"belief_score\"].sample(frac=1, random_state=42).values\n",
        "shuffled_results = train_linear_probe(mean_pooled, shuffled_labels)\n",
        "\n",
        "# Compare early vs late layers for original and shuffled\n",
        "num_layers = mean_pooled.shape[1]\n",
        "early_layer = 0\n",
        "late_layer = num_layers - 1\n",
        "\n",
        "print(\"Original probe metrics (early vs late layer):\")\n",
        "print(test_results.loc[[early_layer, late_layer], [\"layer\", \"pearson\", \"r2\", \"mae\"]])\n",
        "\n",
        "print(\"\\nShuffled probe metrics (early vs late layer):\")\n",
        "print(shuffled_results.loc[[early_layer, late_layer], [\"layer\", \"pearson\", \"r2\", \"mae\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Layer-wise R^2 for original vs shuffled labels\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(test_results[\"layer\"], test_results[\"r2\"], marker=\"o\", label=\"Original\")\n",
        "plt.plot(shuffled_results[\"layer\"], shuffled_results[\"r2\"], marker=\"o\", label=\"Shuffled\")\n",
        "plt.xlabel(\"Layer index\")\n",
        "plt.ylabel(\"R^2\")\n",
        "plt.title(\"Layer-wise Regression Performance\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Pick best layer by R^2 and compute predictions\n",
        "best_layer = int(test_results.sort_values(\"r2\", ascending=False).iloc[0][\"layer\"])\n",
        "X_best = mean_pooled[:, best_layer, :].detach().cpu().numpy()\n",
        "y_true = np.array(dreaddit[\"belief_score\"].values, dtype=float)\n",
        "best_model = LinearRegression()\n",
        "best_model.fit(X_best, y_true)\n",
        "y_pred = best_model.predict(X_best)\n",
        "\n",
        "# Predicted vs true scatter with regression line\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.scatter(y_true, y_pred, alpha=0.5)\n",
        "m, b = np.polyfit(y_true, y_pred, 1)\n",
        "xs = np.linspace(y_true.min(), y_true.max(), 100)\n",
        "plt.plot(xs, m * xs + b, color=\"red\", label=\"Fit line\")\n",
        "plt.xlabel(\"True belief strength\")\n",
        "plt.ylabel(\"Predicted belief strength\")\n",
        "plt.title(f\"Predicted vs True (Layer {best_layer})\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Violin plot of predicted belief strength by group\n",
        "group_labels = np.array(dreaddit[\"category\"].values)\n",
        "pred_depressive = y_pred[group_labels == \"DEPRESSIVE\"]\n",
        "pred_control = y_pred[group_labels == \"CONTROL\"]\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.violinplot([pred_depressive, pred_control], showmeans=True)\n",
        "plt.xticks([1, 2], [\"DEPRESSIVE\", \"CONTROL\"])\n",
        "plt.ylabel(\"Predicted belief strength\")\n",
        "plt.title(\"Predicted Belief Strength by Group\")\n",
        "plt.grid(axis=\"y\", alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Group Level Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# freeze the trained probe\n",
        "# compute predicted belief strength for each text and calculate depressed vs control group means. \n",
        "# then compare distributions, compute effect size, and run a simple statistical test. \n",
        "\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "def group_level_probe(mean_pooled, labels, group_labels, layer_idx=0):\n",
        "    \"\"\"\n",
        "    Use trained probe on a given layer to predict belief strength, compare group means, effect size, and t-test.\n",
        "    Args:\n",
        "        mean_pooled: torch.Tensor [n, num_layers, hidden_dim]\n",
        "        labels: array-like, shape [n]\n",
        "        group_labels: array-like, shape [n] (e.g., 'DEPRESSIVE' or 'CONTROL')\n",
        "        layer_idx: int, layer to use for probe\n",
        "    Returns:\n",
        "        dict with group means, effect size, t-test results\n",
        "    \"\"\"\n",
        "    # Train probe on all data for selected layer\n",
        "    X = mean_pooled[:, layer_idx, :].detach().cpu().numpy()\n",
        "    y = np.array(labels, dtype=float)\n",
        "    model = LinearRegression()\n",
        "    model.fit(X, y)\n",
        "    y_pred = model.predict(X)\n",
        "    # Split by group\n",
        "    group1 = y_pred[np.array(group_labels) == 'DEPRESSIVE']\n",
        "    group2 = y_pred[np.array(group_labels) == 'CONTROL']\n",
        "    mean1 = np.mean(group1)\n",
        "    mean2 = np.mean(group2)\n",
        "    std1 = np.std(group1)\n",
        "    std2 = np.std(group2)\n",
        "    # Cohen's d\n",
        "    pooled_std = np.sqrt(((len(group1)-1)*std1**2 + (len(group2)-1)*std2**2) / (len(group1)+len(group2)-2))\n",
        "    cohens_d = (mean1 - mean2) / pooled_std\n",
        "    # t-test\n",
        "    t_stat, p_val = ttest_ind(group1, group2, equal_var=False)\n",
        "    return {\n",
        "        'DEPRESSIVE_mean': mean1,\n",
        "        'CONTROL_mean': mean2,\n",
        "        \"cohens_d\": cohens_d,\n",
        "        \"t_stat\": t_stat,\n",
        "        \"p_val\": p_val,\n",
        "    }\n",
        "\n",
        "# Run group-level comparison for the last layer (best probe)\n",
        "group_stats = group_level_probe(mean_pooled, dreaddit[\"belief_score\"].values, dreaddit[\"category\"].values, layer_idx=mean_pooled.shape[1]-1)\n",
        "print(\"Group-level comparison (last layer):\")\n",
        "for k, v in group_stats.items():\n",
        "    print(f\"{k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify Similarity of Token-Length Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# addresses maybe longer posts look more hopeless type critiques\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# Compute token lengths for each text\n",
        "text_tokens = dreaddit['text'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
        "dreaddit['token_length'] = text_tokens\n",
        "\n",
        "depressive_lengths = dreaddit.loc[dreaddit['category'] == 'DEPRESSIVE', 'token_length']\n",
        "control_lengths = dreaddit.loc[dreaddit['category'] == 'CONTROL', 'token_length']\n",
        "\n",
        "# Statistical test (Mann-Whitney U, nonparametric)\n",
        "stat, p = mannwhitneyu(depressive_lengths, control_lengths, alternative='two-sided')\n",
        "print(f\"Mann-Whitney U test: statistic={stat}, p-value={p}\")\n",
        "\n",
        "# Plot distributions\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.hist(depressive_lengths, bins=30, alpha=0.6, label='DEPRESSIVE')\n",
        "plt.hist(control_lengths, bins=30, alpha=0.6, label='CONTROL')\n",
        "plt.xlabel('Token Length')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Token Length Distribution by Category')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMMMaYG0JjRcay5US40zYwk",
      "gpuType": "L4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
