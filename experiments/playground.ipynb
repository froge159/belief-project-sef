{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aca3f10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2dc9aeb12e40bfa0f3847e022339fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2027cf7a880478190da6540963d8dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3bf9e66b744077902f92b1b5be8644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f6689238a447f9a5ef9067a1953297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e3248f91e94515b6662e8f48fa2d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ed16a96c8b444e9615ac4fdf9ad2e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bde5359d6034675b45a006f95477169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324e8f0520424f268dd7a77fb86c8d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6117c1f9dc7d493884bb044baf2bfb49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17708fd93d574184b0e2fadaa5fa0714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35922a9293349a6a832a35e2d87d1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load base model and tokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_id = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id, dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30dd21bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze model weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a05800de",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"The fresh powder on the mountain was incredible.\",\n",
    "    \"Avoid the gaper gap at all costs.\",\n",
    "    \"Après-ski is the best part of the day.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a27872e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tokenization Sanity Check ---\n",
      "Token IDs:\n",
      "tensor([[   785,   7722,  16723,    389,    279,  16301,    572,  15050,     13,\n",
      "         151643, 151643, 151643],\n",
      "        [ 52116,    279,    342,   3191,  12929,    518,    678,   7049,     13,\n",
      "         151643, 151643, 151643],\n",
      "        [ 20722,  12142,   1331,   6642,    374,    279,   1850,    949,    315,\n",
      "            279,   1899,     13]], device='cuda:0')\n",
      "Decoded Text:\n",
      "['The fresh powder on the mountain was incredible.', 'Avoid the gaper gap at all costs.', 'Après-ski is the best part of the day.']\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer(examples, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "print(\"--- Tokenization Sanity Check ---\")\n",
    "print(f\"Token IDs:\\n{inputs['input_ids']}\")\n",
    "print(f\"Decoded Text:\\n{tokenizer.batch_decode(inputs['input_ids'], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d73636d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Hidden State Metadata ---\n",
      "Number of layers (including embedding): 37\n",
      "Shape of each layer tensor: torch.Size([3, 12, 2560])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "hidden_states = outputs.hidden_states\n",
    "print(\"--- Hidden State Metadata ---\")\n",
    "print(f\"Number of layers (including embedding): {len(hidden_states)}\")\n",
    "print(f\"Shape of each layer tensor: {hidden_states[0].shape}\") # [Batch, Seq_Len, Hidden_Size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a19139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer = hidden_states[-1]\n",
    "sequence_lengths = inputs['attention_mask'].sum(dim=1) - 1\n",
    "final_token_vectors = last_layer[torch.arange(last_layer.size(0)), sequence_lengths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06472941",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = inputs['attention_mask'].unsqueeze(-1).expand(last_layer.size()).float()\n",
    "sum_embeddings = torch.sum(last_layer * mask, dim=1)\n",
    "sum_mask = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "mean_pooled_vectors = sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a3a9024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Extraction Complete ---\n",
      "Final Token Vectors Shape: (3, 2560)\n",
      "Mean Pooled Vectors Shape: (3, 2560)\n",
      "Files saved: final_token_vectors.npy, mean_pooled_vectors.npy\n"
     ]
    }
   ],
   "source": [
    "final_token_np = final_token_vectors.cpu().numpy()\n",
    "mean_pooled_np = mean_pooled_vectors.cpu().numpy()\n",
    "np.save(\"final_token_vectors.npy\", final_token_np)\n",
    "np.save(\"mean_pooled_vectors.npy\", mean_pooled_np)\n",
    "\n",
    "print(\"--- Extraction Complete ---\")\n",
    "print(f\"Final Token Vectors Shape: {final_token_np.shape}\")\n",
    "print(f\"Mean Pooled Vectors Shape: {mean_pooled_np.shape}\")\n",
    "print(\"Files saved: final_token_vectors.npy, mean_pooled_vectors.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b867f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3Model(\n",
       "  (embed_tokens): Embedding(151936, 2560)\n",
       "  (layers): ModuleList(\n",
       "    (0-35): 36 x Qwen3DecoderLayer(\n",
       "      (self_attn): Qwen3Attention(\n",
       "        (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "        (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "      )\n",
       "      (mlp): Qwen3MLP(\n",
       "        (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "        (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "        (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "  (rotary_emb): Qwen3RotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41944f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(text: str) -> dict:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    states = outputs.hidden_states\n",
    "    hidden_states_dict = {\n",
    "        f\"layer_{i}\": layer.squeeze(0).cpu().numpy()\n",
    "        for i, layer in enumerate(states)\n",
    "    }\n",
    "    hidden_states_dict[\"input_tokens\"] = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    return hidden_states_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d717087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers extracted: 37\n",
      "Shape of last layer: (8, 2560)\n",
      "Tokens: ['S', 'ki', 'ing', 'Ġin', 'Ġthe', 'Ġfresh', 'Ġpowder', '.']\n"
     ]
    }
   ],
   "source": [
    "data = extract_hidden_states(\"Skiing in the fresh powder.\")\n",
    "print(f\"Layers extracted: {len(data) - 1}\") # Minus 1 because of the 'input_tokens' key\n",
    "print(f\"Shape of last layer: {data['layer_12'].shape}\") # [Seq_Len, Hidden_Dim]\n",
    "print(f\"Tokens: {data['input_tokens']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
